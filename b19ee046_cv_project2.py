# -*- coding: utf-8 -*-
"""B19EE046_CV_Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-uQwHke4zxsEf1dEMETsE-TMhYl_7VW

#Pre-Requisites
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install pytorch_lightning

import os
import torch
from torch.utils.data import Dataset, random_split, DataLoader
import pytorch_lightning as pl
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from torchvision import models
import PIL
from tqdm import tqdm
import cv2
import pandas as pd
from sklearn.metrics import classification_report, roc_curve
import torchmetrics

from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.ensemble import BaggingClassifier as BC
from sklearn.ensemble import AdaBoostClassifier as ABC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score
from sklearn.decomposition import PCA
from sklearn.svm import SVC

def transform_ml_fn(img):
  img = img.resize((100,100))
  tensor = transforms.ToTensor()(img)
  return tensor

def feature_detector(data, feature_mode='sift', num_features=5):
  data = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY).astype('uint8')
  if feature_mode=='sift':
    fd = cv2.xfeatures2d.SIFT_create()
  elif feature_mode=='orb':
    fd = cv2.ORB_create()
  elif feature_mode=='akaze':
    fd = cv2.AKAZE_create()
  else:
    print("Its not available")
    return 0
  des = fd.detectAndCompute(data,None)[1]
  if des is not None and des.shape[0] > num_features:
      des = des[:num_features]
  elif des is None:
    if feature_mode=='sift':
      des = np.zeros((num_features, 128))
    if feature_mode=='orb':
      des = np.zeros((num_features, 32))
    if feature_mode=='akaze':
      des = np.zeros((num_features, 61))
  else:
    if feature_mode=='sift':
      des = np.vstack((des, np.zeros((num_features - des.shape[0], 128))))
    if feature_mode=='orb':
      des = np.vstack((des, np.zeros((num_features - des.shape[0], 32))))
    if feature_mode=='akaze':
      des = np.vstack((des, np.zeros((num_features - des.shape[0], 61))))
  return des.reshape(-1)

def model_return(ckpt_path, model_obj):
  ckpt = torch.load(ckpt_path)
  model_obj.load_state_dict(ckpt['state_dict'])
  return model_obj.eval().cuda()

def accuracy(pred,true):
  class_vector = np.zeros((1,26))
  total_vector = np.zeros((1,26))
  total_correct = 0
  for i in range(len(pred)):
    if pred[i]==true[i]:
      total_correct+=1
      class_vector[0][true[i]]+=1
    total_vector[0][true[i]]+=1
  print("\nOverall Accuracy = ", 100*total_correct/len(pred), "%")
  print("\nClass-Wise Accuracies = ", 100*class_vector/total_vector)
  print(classification_report(pred,true))

def detect(test_loader, model_class):
  pred=[]
  true=[]
  proba=[]
  for i,j in tqdm(test_loader):
    i,j = i.cuda(), j.cuda()
    prob = model_class(i)
    out = prob.max(1, keepdim=True)[1]
    pred.append(out.detach().cpu().item())
    true.append(j.detach().cpu().item())
    proba.append(prob.detach().cpu().numpy())
  print("Confusion Matrix:\n")
  confusion_matrix(pred, true)
  accuracy(pred, true)
  del pred
  MultiClass_ROC(np.array(proba), true)
  del proba, true

def MultiClass_ROC(proba, true):
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = 26
  plt.figure(figsize=(10,10))
  for i in range(n_class):
    fpr[i], tpr[i], thresh[i] = roc_curve(true, [proba[j][0,i] for j in range(proba.shape[0])], pos_label=i)
    plt.plot(fpr[i], tpr[i], label='Class '+str(i)+' vs Rest')
  plt.title('Multiclass ROC curve')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()

class Model(pl.LightningModule):

  def __init__(self, model_name, num_classes, loss_mode='normal'):
    super().__init__()
    self.model_name = model_name
    self.num_classes = num_classes
    if self.model_name=='resnet_34':
      self.model = models.resnet34(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='convnext_tiny':
      self.model = models.convnext_tiny(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    if self.model_name=='googlenet':
      self.model = models.googlenet(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='mnasnet0_5':
      self.model = models.mnasnet0_5(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    if self.model_name=='regnet_y_400mf':
      self.model = models.regnet_y_400mf(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='resnext50_32x4d':
      self.model = models.resnext50_32x4d(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='shufflenet_v2_x0_5':
      self.model = models.shufflenet_v2_x0_5(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='squeezenet1_0':
      self.model = models.squeezenet1_0(weights=None)
      self.features = self.model.classifier[-3].in_channels
      self.model.classifier[-3] = nn.Conv2d(self.features, self.num_classes, kernel_size=(1, 1), stride=(1, 1))
    if self.model_name=='vgg11':
      self.model = models.vgg11(weights=None)
      self.features = self.model.classifier[-1].in_features 
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    if self.model_name=='wide_resnet50_2':
      self.model = models.wide_resnet50_2(weights=None)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,self.num_classes)
    if self.model_name=='swin_t':
      self.model = models.swin_t(weights=None)
      self.features = self.model.head.in_features
      self.model.head = nn.Linear(self.features,self.num_classes)
    if self.model_name=='densenet_121':
      self.model = models.densenet121(weights=None)
      self.features = self.model.classifier.in_features
      self.model.classifier = nn.Linear(self.features,self.num_classes)
    if self.model_name=='alexnet':
      self.model = models.alexnet(weights=None)
      self.features = self.model.classifier[-1].in_features
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    if self.model_name=='efficientnet_b0':
      self.model = models.efficientnet.efficientnet_b0(weights=None)
      self.features = self.model.classifier[-1].in_features
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    if self.model_name=='mobilenet_v3_large':
      self.model = models.mobilenet_v3_large(weights=None)
      self.features = self.model.classifier[-1].in_features
      self.model.classifier[-1] = nn.Linear(self.features,self.num_classes)
    self.loss_mode = loss_mode
    if self.loss_mode=='normal':
      self.loss = nn.CrossEntropyLoss()
    if self.loss_mode=='arcface':
      self.loss = ArcFaceLoss()
    if self.loss_mode=='cosface':
      self.loss = CosFaceLoss(num_classes=self.num_classes, embedding_size=self.features)
    if self.loss_mode=='adaptivefaceloss':
      self.loss = AdaptiveFaceLoss(num_classes=self.num_classes, embedding_size=self.features, scale=30.0)

  def forward(self, x):
    return self.model(x)

  def training_step(self, batch, batch_no):
    x, y = batch
    if self.model_name=='googlenet':
      logits = self(x).logits
    else:
      logits = self(x)
    loss = self.loss(logits, y)
    return loss

  def configure_optimizers(self):
    return torch.optim.Adam(self.parameters(),lr=0.001)

class CosFaceLoss(nn.Module):
    def __init__(self, num_classes, embedding_size, s=30.0, m=0.35):
        super().__init__()
        self.num_classes = num_classes
        self.embedding_size = embedding_size
        self.s = s
        self.m = m
        self.weights = nn.Parameter(torch.randn(embedding_size, num_classes))

    def forward(self, embeddings, targets):
        cosine = F.linear(F.normalize(embeddings), F.normalize(self.weights))
        one_hot = torch.zeros_like(cosine)
        one_hot.scatter_(1, targets.view(-1, 1), 1)
        logits = self.s * (cosine - one_hot * self.m)
        return F.cross_entropy(logits, targets)

class ArcFaceLoss(nn.Module):
    def __init__(self, s=30.0, m=0.5):
        super().__init__()
        self.s = s
        self.m = m

    def forward(self, logits, labels):
        logits = logits.float()
        cosine = logits / torch.norm(logits, dim=1, keepdim=True)
        theta = torch.acos(cosine)
        one_hot = F.one_hot(labels, num_classes=logits.size(1)).float()
        target_logits = torch.cos(theta + self.m)
        logits = self.s * torch.where(one_hot == 1, target_logits, cosine)
        loss = F.cross_entropy(logits, labels)
        return loss

class AdaptiveFaceLoss(nn.Module):
    def __init__(self, num_classes, embedding_size, scale=30.0):
        super(AdaptiveFaceLoss, self).__init__()
        self.num_classes = num_classes
        self.embedding_size = embedding_size
        self.scale = scale
        self.margin = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))
        nn.init.xavier_uniform_(self.margin)

    def forward(self, logits, targets):
        logits = logits.float()
        targets = targets.long().view(-1, 1).to(logits.device)
        batch_size = logits.size(0)
        mask = targets.expand(batch_size, self.num_classes).eq(torch.arange(self.num_classes).expand(batch_size, -1).to(logits.device))
        cos_theta = logits
        sin_theta = torch.sqrt(1.0 - torch.pow(cos_theta, 2))
        cos_theta_m = cos_theta * self.margin
        sin_theta_m = torch.sqrt(1.0 - torch.pow(cos_theta_m, 2))
        idx = torch.arange(0, batch_size)
        cos_theta_m[idx, targets] = cos_theta[idx, targets]
        sin_theta_m[idx, targets] = sin_theta[idx, targets]
        cos_theta_m = cos_theta_m * (1 - mask.float()) + cos_theta * mask.float()
        sin_theta_m = sin_theta_m * (1 - mask.float()) + sin_theta * mask.float()
        cos_theta_m = cos_theta_m - self.scale * mask.float()
        loss = F.cross_entropy(torch.cat((cos_theta_m, sin_theta_m), dim=1), targets.view(-1))
        return loss

"""#Traditional Machine Learning Approach (LFW)"""

train_data = datasets.LFWPeople('../data', split='train', download=True, transform=transform_ml_fn)
test_data = datasets.LFWPeople('../data', split='test', download=True, transform=transform_ml_fn)

train_data_load = torch.utils.data.DataLoader(train_data, batch_size=25,shuffle=True)
test_data_load = torch.utils.data.DataLoader(test_data, batch_size=1,shuffle=True)

"""##Just Flatten"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x_train.append(x)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x_test.append(x0)
  y_test.append(y0)

x_train = torch.stack(x_train).numpy()
y_train = torch.stack(y_train).numpy()
x_test = torch.stack(x_test).numpy()
y_test = torch.stack(y_test).numpy()

x_train = x_train.reshape((x_train.shape[0],-1))
y_train = y_train.reshape((y_train.shape[0],-1))
x_test = x_test.reshape((x_test.shape[0],-1))
y_test = y_test.reshape((y_test.shape[0],-1))

x_train.shape

"""### For Grayscaled PCA with 0.9 as variance"""

PCA_train = PCA(n_components=0.9)
PCA_train.fit(x_train)

print(PCA_train.transform(x_train).shape)

x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))

model_3 = BC()
model_3.fit(x_train,y_train)
print(model_3.score(x_test,y_test))

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))

"""### For RGB PCA with 186 as n_components"""

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)

x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)

x_train.shape

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))

model_3 = BC()
model_3.fit(x_train,y_train)
print(model_3.score(x_test,y_test))

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))

"""##Feature Descriptor (SIFT)

### Num_Features = 1 (128 dimensional each sample)
"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'sift',1)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'sift',1)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, x_test, y_train, y_test

"""### Num_Features = 5"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'sift',5)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'sift',5)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 25"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'sift',25)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'sift',25)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 100"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'sift',100)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'sift',100)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""##Feature Descriptor (ORB)

### Num_Features = 1 (32 dimensional each sample)
"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'orb',1)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'orb',1)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, x_test, y_train, y_test

"""### Num_Features = 5"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'orb',5)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'orb',5)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 25"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'orb',25)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'orb',25)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 100"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'orb',100)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'orb',100)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""##Feature Descriptor (AKAZE)

### Num_Features = 1 (61 dimensional each sample)
"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'akaze',1)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'akaze',1)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, x_test, y_train, y_test

"""### Num_Features = 5"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'akaze',5)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'akaze',5)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 25"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'akaze',25)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'akaze',25)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""### Num_Features = 100"""

x_train=[]
y_train=[]
x_test=[]
y_test=[]

for x,y in tqdm(train_data_load):
  x = torch.squeeze(x,dim=0)
  x = torch.permute(x,[1,2,0])*255
  des = feature_detector(x.numpy(),'akaze',100)
  x_train.append(des)
  y_train.append(y)

for x0,y0 in tqdm(test_data_load):
  x0 = torch.squeeze(x0,dim=0)
  x0 = torch.permute(x0,[1,2,0])*255
  des = feature_detector(x0.numpy(),'akaze',100)
  x_test.append(des)
  y_test.append(y0)

x_train = np.array(x_train)
y_train = torch.stack(y_train).numpy()
x_test = np.array(x_test)
y_test = torch.stack(y_test).numpy()

PCA_train = PCA(n_components=186)
PCA_train.fit(x_train)
x_train = PCA_train.transform(x_train)
x_test = PCA_train.transform(x_test)
del PCA_train

model_1 = DTC()
model_1.fit(x_train,y_train)
print(model_1.score(x_test,y_test))
del model_1

model_2 = RFC(n_estimators = 18)
model_2.fit(x_train,y_train)
print(model_2.score(x_test,y_test))
del model_2

model_4 = ABC()
model_4.fit(x_train,y_train)
print(model_4.score(x_test,y_test))
del model_4

model_5 = SVC(kernel='rbf')
model_5.fit(x_train,y_train)
print(model_5.score(x_test,y_test))
del model_5, x_train, y_train, x_test, y_test

"""# Traditional CNNs Approach (LFW)"""

alexnet = Model('alexnet',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/alexnet', benchmark=True)
trainer.fit(alexnet, train_data_load)
del trainer, alexnet

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/alexnet/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('alexnet',5749))
detect(test_data_load, Model_Test)

vgg11 = Model('vgg11',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/vgg11', benchmark=True)
trainer.fit(vgg11, train_data_load)
del trainer, vgg11

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/vgg11/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('vgg11',5749))
detect(test_data_load, Model_Test)

googlenet = Model('googlenet',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/googlenet', benchmark=True)
trainer.fit(googlenet, train_data_load)
del trainer, googlenet

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/googlenet/lightning_logs/version_5/checkpoints/epoch=19-step=7620.ckpt', Model('googlenet',5749))
detect(test_data_load, Model_Test)

resnet_34 = Model('resnet_34',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/resnet_34', benchmark=True)
trainer.fit(resnet_34, train_data_load)
del trainer, resnet_34

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/resnet_34/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('resnet_34',5749))
detect(test_data_load, Model_Test)

densenet_121 = Model('densenet_121',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/densenet_121', benchmark=True)
trainer.fit(densenet_121, train_data_load)
del trainer, densenet_121

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/densenet_121/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('densenet_121',5749))
detect(test_data_load, Model_Test)

wide_resnet50_2 = Model('wide_resnet50_2',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/wide_resnet50_2', benchmark=True)
trainer.fit(wide_resnet50_2, train_data_load)
del trainer, wide_resnet50_2

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/wide_resnet50_2/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('wide_resnet50_2',5749))
detect(test_data_load, Model_Test)

resnext50_32x4d = Model('resnext50_32x4d',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/resnext50_32x4d', benchmark=True)
trainer.fit(resnext50_32x4d, train_data_load)
del trainer, resnext50_32x4d

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/resnext50_32x4d/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('resnext50_32x4d',5749))
detect(test_data_load, Model_Test)

squeezenet1_0 = Model('squeezenet1_0',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/squeezenet1_0', benchmark=True)
trainer.fit(squeezenet1_0, train_data_load)
del trainer, squeezenet1_0

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/squeezenet1_0/lightning_logs/version_1/checkpoints/epoch=19-step=7620.ckpt', Model('squeezenet1_0',5749))
detect(test_data_load, Model_Test)

convnext_tiny = Model('convnext_tiny',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/convnext_tiny', benchmark=True)
trainer.fit(convnext_tiny, train_data_load)
del trainer, convnext_tiny

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/convnext_tiny/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('convnext_tiny',5749))
detect(test_data_load, Model_Test)

shufflenet_v2_x0_5 = Model('shufflenet_v2_x0_5',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/shufflenet_v2_x0_5', benchmark=True)
trainer.fit(shufflenet_v2_x0_5, train_data_load)
del trainer, shufflenet_v2_x0_5

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/shufflenet_v2_x0_5/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('shufflenet_v2_x0_5',5749))
detect(test_data_load, Model_Test)

mobilenet_v3_large = Model('mobilenet_v3_large',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/mobilenet_v3_large', benchmark=True)
trainer.fit(mobilenet_v3_large, train_data_load)
del trainer, mobilenet_v3_large

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/mobilenet_v3_large/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('mobilenet_v3_large',5749))
detect(test_data_load, Model_Test)

efficientnet_b0 = Model('efficientnet_b0',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/efficientnet_b0', benchmark=True)
trainer.fit(efficientnet_b0, train_data_load)
del trainer, efficientnet_b0

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/efficientnet_b0/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('efficientnet_b0',5749))
detect(test_data_load, Model_Test)

swin_t = Model('swin_t',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/swin_t', benchmark=True)
trainer.fit(swin_t, train_data_load)
del trainer, swin_t

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/swin_t/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('swin_t',5749))
detect(test_data_load, Model_Test)

mnasnet0_5 = Model('mnasnet0_5',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/mnasnet0_5', benchmark=True)
trainer.fit(mnasnet0_5, train_data_load)
del trainer, mnasnet0_5

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/mnasnet0_5/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('mnasnet0_5',5749))
detect(test_data_load, Model_Test)

regnet_y_400mf = Model('regnet_y_400mf',5749).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Project_2/regnet_y_400mf', benchmark=True)
trainer.fit(regnet_y_400mf, train_data_load)
del trainer, regnet_y_400mf

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/regnet_y_400mf/lightning_logs/version_0/checkpoints/epoch=19-step=7620.ckpt', Model('regnet_y_400mf',5749))
detect(test_data_load, Model_Test)

"""# Traditional CNN Approach (IMFDB)"""

direct = '/content/drive/MyDrive/CV_Project_2/IMFDB'
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor()
])

dataset = datasets.ImageFolder(direct, transform=transform)
print(len(dataset))

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

"""## Binary Cross Entropy"""

mobilenet_v3_large = Model('mobilenet_v3_large',26).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/mobilenet_v3_large', benchmark=True)
trainer.fit(mobilenet_v3_large, train_loader)
del trainer, mobilenet_v3_large

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/mobilenet_v3_large/lightning_logs/version_0/checkpoints/epoch=14-step=28065.ckpt', Model('mobilenet_v3_large',26))
detect(test_loader, Model_Test)
del Model_Test

squeezenet1_0 = Model('squeezenet1_0',26).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/squeezenet1_0', benchmark=True)
trainer.fit(squeezenet1_0, train_loader)
del trainer, squeezenet1_0

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/squeezenet1_0/lightning_logs/version_0/checkpoints/epoch=14-step=28065.ckpt', Model('squeezenet1_0',26))
detect(test_loader, Model_Test)
del Model_Test

shufflenet_v2_x0_5 = Model('shufflenet_v2_x0_5',26).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5', benchmark=True)
trainer.fit(shufflenet_v2_x0_5, train_loader)
del trainer, shufflenet_v2_x0_5

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5/lightning_logs/version_0/checkpoints/epoch=14-step=28065.ckpt', Model('shufflenet_v2_x0_5',26))
detect(test_loader, Model_Test)
del Model_Test

resnet_34 = Model('resnet_34',26).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34', benchmark=True)
trainer.fit(resnet_34, train_loader)
del trainer, resnet_34

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34/lightning_logs/version_0/checkpoints/epoch=14-step=28065.ckpt', Model('resnet_34',26))
detect(test_loader, Model_Test)
del Model_Test

densenet_121 = Model('densenet_121',26).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121', benchmark=True)
trainer.fit(densenet_121, train_loader)
del trainer, densenet_121

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121/lightning_logs/version_0/checkpoints/epoch=14-step=28065.ckpt', Model('densenet_121',26))
detect(test_loader, Model_Test)
del Model_Test

"""## Other Losses (on Shufflenet_v2_x0_5, ResNet_34 and DenseNet_121)

### ShuffleNet
"""

shufflenet_v2_x0_5 = Model('shufflenet_v2_x0_5',26,'arcface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5', benchmark=True)
trainer.fit(shufflenet_v2_x0_5, train_loader)
del trainer, shufflenet_v2_x0_5

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5/lightning_logs/version_1/checkpoints/epoch=14-step=28065.ckpt', Model('shufflenet_v2_x0_5',26,'arcface'))
detect(test_loader, Model_Test)
del Model_Test

shufflenet_v2_x0_5 = Model('shufflenet_v2_x0_5',26,'cosface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5', benchmark=True)
trainer.fit(shufflenet_v2_x0_5, train_loader)
del trainer, shufflenet_v2_x0_5

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/shufflenet_v2_x0_5/lightning_logs/version_4/checkpoints/epoch=14-step=28065.ckpt', Model('shufflenet_v2_x0_5',26,'cosface'))
detect(test_loader, Model_Test)
del Model_Test

"""### ResNet"""

resnet_34 = Model('resnet_34',26,'arcface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34', benchmark=True)
trainer.fit(resnet_34, train_loader)
del trainer, resnet_34

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34/lightning_logs/version_1/checkpoints/epoch=14-step=28065.ckpt', Model('resnet_34',26,'arcface'))
detect(test_loader, Model_Test)
del Model_Test

resnet_34 = Model('resnet_34',26,'cosface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34', benchmark=True)
trainer.fit(resnet_34, train_loader)
del trainer, resnet_34

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/resnet_34/lightning_logs/version_2/checkpoints/epoch=14-step=28065.ckpt', Model('resnet_34',26,'cosface'))
detect(test_loader, Model_Test)
del Model_Test

"""### DenseNet"""

densenet_121 = Model('densenet_121',26,'arcface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121', benchmark=True)
trainer.fit(densenet_121, train_loader)
del trainer, densenet_121

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121/lightning_logs/version_1/checkpoints/epoch=14-step=28065.ckpt', Model('densenet_121',26,'arcface'))
detect(test_loader, Model_Test)
del Model_Test

densenet_121 = Model('densenet_121',26,'cosface').train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=15, default_root_dir='/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121', benchmark=True)
trainer.fit(densenet_121, train_loader)
del trainer, densenet_121

Model_Test = model_return('/content/drive/MyDrive/CV_Project_2/Models_IMFDB/densenet_121/lightning_logs/version_2/checkpoints/epoch=12-step=24323.ckpt', Model('densenet_121',26,'cosface'))
detect(test_loader, Model_Test)
del Model_Test